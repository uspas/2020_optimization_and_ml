{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09 - Reinforcement Learning\n",
    "\n",
    "In reinforcement learning (RL), an \"agent\" chooses actions given information about the present state of the system, and is given a reward (a measure of how good the action was). The aim is to maximize the cumulative returned reward over many interactions with the system.\n",
    "\n",
    "The problem is defined as a series of state transitions consisting of: State --> Action --> New State + reward.\n",
    "\n",
    "Multiple sequences make up an \"episode\" consisting of N state transitions.\n",
    "\n",
    "</pre>\n",
    "<img src=\"notebook_images/RL.png\" width=\"600\"> \n",
    "<pre>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll look at the Deep Deterministic Policy Gradient (DDPG) algorithm (see paper [here](https://arxiv.org/abs/1509.02971)), for two cases:\n",
    "1. The standard pendulum problem: see https://gym.openai.com/envs/Pendulum-v0/\n",
    "\n",
    "\n",
    "2. Tuning 3 quadrupoles in an accelerator lattice to minimize beam size. The actions are the changes in quad settings. The state is the present beamsize output and quad settings.\n",
    "\n",
    "DDPG is an example of actor-critic RL where a mapping from observed to actions (i.e. a  \"policy\" or \"actor\") and an estimate of the long-term value of taking actions in a given state (i.e. a \"critic\") are both learned functions parameterized by neural networks. The critic provides the training signal for the actor, and the returned rewards provide the training signal for the critic.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/jupyter/lab9/Round-to-Flat-Beam-RL\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "import sys\n",
    "sys.path.append('/home/vagrant/jupyter/lab9/Round-to-Flat-Beam-RL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/vagrant/.pyenv/versions/3.7.2/envs/py3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/uspas/2021_optimization_and_ml --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (0.18.3)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from gym) (1.5.15)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from gym) (1.20.3)\n",
      "Requirement already satisfied: scipy in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from gym) (1.6.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in /home/vagrant/.pyenv/versions/3.7.2/envs/py3/lib/python3.7/site-packages (from gym) (8.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/vagrant/.pyenv/versions/3.7.2/envs/py3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "from os import path\n",
    "\n",
    "#import toy accelerator package\n",
    "from uspas_ml.accelerator_toy_models import simple_lattices\n",
    "from uspas_ml.utils import transformer\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at the DDPGAgent class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg.models import Critic, Actor\n",
    "from common.exp_replay import ExperienceReplayLog\n",
    "from common.noise import OUNoise\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    \n",
    "    def __init__(self, env, gamma, tau, buffer_maxlen, critic_learning_rate, actor_learning_rate):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.noise = OUNoise(env.action_space)\n",
    "        self.iter = 0.0\n",
    "        self.noisy = False\n",
    "        \n",
    "        #print(self.action_dim)\n",
    "        #print(self.obs_dim)\n",
    "        \n",
    "        # RL hyperparameters\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Initialize critic and actorr networks\n",
    "        self.critic = Critic(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.critic_target = Critic(self.obs_dim, self.action_dim).to(self.device)\n",
    "        \n",
    "        self.actor = Actor(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.actor_target = Actor(self.obs_dim, self.action_dim).to(self.device)\n",
    "    \n",
    "        # Copy target network paramters for critic\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Set Optimization algorithms\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n",
    "        self.actor_optimizer  = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n",
    "    \n",
    "        self.replay_buffer = ExperienceReplayLog(buffer_maxlen)        \n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        #print('obs;',obs)\n",
    "        \n",
    "        if self.noisy == True:\n",
    "            state = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "            action = self.actor.forward(state)\n",
    "            action = action.squeeze(0).cpu().detach().numpy()\n",
    "            action = self.noise.get_action(action,self.iter)\n",
    "            self.iter = self.iter+1\n",
    "        \n",
    "        else:\n",
    "            state = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "            action = self.actor.forward(state)\n",
    "            action = action.squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        \n",
    "        #Batch updates\n",
    "        states, actions, rewards, next_states, _ = self.replay_buffer.sample(batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, masks = self.replay_buffer.sample(batch_size)\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        masks = torch.FloatTensor(masks).to(self.device)\n",
    "   \n",
    "        # Q info updates\n",
    "        curr_Q = self.critic.forward(state_batch, action_batch)\n",
    "        next_actions = self.actor_target.forward(next_state_batch)\n",
    "        next_Q = self.critic_target.forward(next_state_batch, next_actions.detach())\n",
    "        expected_Q = reward_batch + self.gamma * next_Q\n",
    "        \n",
    "        # Update Critic network\n",
    "        q_loss = F.mse_loss(curr_Q, expected_Q.detach())\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        q_loss.backward() \n",
    "        \n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update Actor network\n",
    "        policy_loss = -self.critic.forward(state_batch, self.actor.forward(state_batch)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update Actor and Critic target networks \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "       \n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Take a look at how the Pendulum environment is defined in openAI gym below (we've added additional helper functions for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PendulumEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 30\n",
    "    }\n",
    "\n",
    "    def __init__(self, g=10.0):\n",
    "        self.max_speed = 8\n",
    "        self.max_torque = 2.\n",
    "        self.dt = .05\n",
    "        self.g = g\n",
    "        self.m = 1.\n",
    "        self.l = 1.\n",
    "        self.viewer = None\n",
    "\n",
    "        self.costs = []\n",
    "        self.a1 = []\n",
    "        high = np.array([1., 1., self.max_speed], dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-self.max_torque,\n",
    "            high=self.max_torque, shape=(1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-high,\n",
    "            high=high,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.seed()\n",
    "        self.q1s=[]\n",
    "        self.costs=[]\n",
    "        self.reset_ep=[]\n",
    "        self.plot = True\n",
    "        \n",
    "\n",
    "    def log_results(self,q11, reward, reset_ep_flag  = False):\n",
    "        \n",
    "        '''\n",
    "        logs the results of a given iteration, for quad inputs, the reward, and whether the episode was resett\n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.costs.append(reward)\n",
    "        self.q1s.append(q11)\n",
    "\n",
    "        \n",
    "        \n",
    "        if reset_ep_flag == True:\n",
    "            self.reset_ep.append(1)\n",
    "            \n",
    "        else:\n",
    "            self.reset_ep.append(0)\n",
    "\n",
    "    def plot_results(self,):\n",
    "        \n",
    "        '''\n",
    "        plots the results from the logged values\n",
    "        \n",
    "        '''\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "\n",
    "        f = plt.figure(figsize=(25,3))\n",
    "        ax = f.add_subplot(141)\n",
    "        ax2 = f.add_subplot(142)\n",
    "        \n",
    "            \n",
    "        plot_reset = np.where(np.array(self.reset_ep)==1)[0]\n",
    "        for i in range(0, len(plot_reset)):\n",
    "            ax.axvline(x=plot_reset[i],alpha=0.1,color='k')\n",
    "            ax2.axvline(x=plot_reset[i],alpha=0.1,color='k')\n",
    "                \n",
    "        ax.plot(self.q1s,'.')\n",
    "        ax.set_ylabel('Q1',fontsize=12)\n",
    "        ax2.plot(self.costs, 'k.')\n",
    "        ax2.set_ylabel('reward',fontsize=12)\n",
    "        \n",
    "        ax.set_xlabel('Iteration',fontsize=12)\n",
    "        ax2.set_xlabel('Iteration',fontsize=12)\n",
    "\n",
    "\n",
    "            \n",
    "        plt.show();\n",
    "        \n",
    "    def reset_plot(self,):\n",
    "        \n",
    "        '''\n",
    "        resets the logs and the plot\n",
    "        \n",
    "        '''\n",
    "        self.costs=[]\n",
    "        self.q1s=[]\n",
    "        self.reset_ep=[]\n",
    "        \n",
    "        \n",
    "    def save_plot(self,name = 'mon_'):\n",
    "        \n",
    "        '''\n",
    "        saves results from the logged values to a json file\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        run_details = {\n",
    "            'q1': self.q1s,\n",
    "            'costs': self.costs,\n",
    "            'reset_ep': self.reset_ep,\n",
    "        } \n",
    "\n",
    "        with open(name + '.json', 'w') as json_file:\n",
    "            json.dump(run_details, json_file, default = to_serializable)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, u):\n",
    "        th, thdot = self.state  # th := theta\n",
    "\n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        dt = self.dt\n",
    "\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u  # for rendering\n",
    "        costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)\n",
    "\n",
    "        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt\n",
    "        newth = th + newthdot * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.log_results(u, -costs, reset_ep_flag = False)\n",
    "        \n",
    "    \n",
    "        if self.plot == True:\n",
    "            self.plot_results()\n",
    "\n",
    "        else:\n",
    "            clear_output()\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "        return self._get_obs(), -costs, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        high = np.array([np.pi, 1])\n",
    "        self.state = self.np_random.uniform(low=-high, high=high)\n",
    "        self.last_u = None\n",
    "        \n",
    "        self.log_results(np.nan, np.nan, reset_ep_flag = True)\n",
    "        \n",
    "    \n",
    "        if self.plot == True:\n",
    "            self.plot_results()\n",
    "\n",
    "        else:\n",
    "            clear_output()\n",
    "\n",
    "        \n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot])\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(500, 500)\n",
    "            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)\n",
    "            rod = rendering.make_capsule(1, .2)\n",
    "            rod.set_color(.8, .3, .3)\n",
    "            self.pole_transform = rendering.Transform()\n",
    "            rod.add_attr(self.pole_transform)\n",
    "            self.viewer.add_geom(rod)\n",
    "            axle = rendering.make_circle(.05)\n",
    "            axle.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(axle)\n",
    "            fname = path.join(path.dirname(__file__), \"assets/clockwise.png\")\n",
    "            self.img = rendering.Image(fname, 1., 1.)\n",
    "            self.imgtrans = rendering.Transform()\n",
    "            self.img.add_attr(self.imgtrans)\n",
    "\n",
    "        self.viewer.add_onetime(self.img)\n",
    "        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)\n",
    "        if self.last_u:\n",
    "            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return (((x+np.pi) % (2*np.pi)) - np.pi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the agent and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from common.utils import mini_batch_train\n",
    "from ddpg.ddpg import DDPGAgent\n",
    "\n",
    "#initialize environment\n",
    "env = PendulumEnv() # alternative: gym.make(\"Pendulum-v0\")\n",
    "\n",
    "max_episodes = 100 \n",
    "max_steps = 100 #max number of steps per episode\n",
    "batch_size = 300 #batch size for updates\n",
    "buffer_maxlen = 100000 #max buffer size\n",
    "\n",
    "# define training hyperparameters\n",
    "gamma = 0.99 #discount factor\n",
    "tau = 1e-2  #for updates with target network\n",
    "\n",
    "critic_lr = 1e-3 #learning rate\n",
    "actor_lr = 1e-3 #learning rate\n",
    "\n",
    "#initialize agent\n",
    "agent = DDPGAgent(env, gamma, tau, buffer_maxlen, critic_lr, actor_lr)\n",
    "\n",
    "#train with mini-batches\n",
    "episode_rewards = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faster without plotting\n",
    "env.plot=False\n",
    "episode_rewards = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results\n",
    "env.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest plot and see how training is going with live plotting\n",
    "env.reset_plot()\n",
    "env.plot=True\n",
    "episode_rewards = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
