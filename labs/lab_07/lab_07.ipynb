{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07 - Uncertainty Quantification\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- Train different machine learning algorithms on noisy data\n",
    "- Predict the uncertainty from the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/uspas/2021_optimization_and_ml --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blitz-bayesian-pytorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm # For progress bar\n",
    "#matplotlib graphs will be included in your notebook, next to the code:\n",
    "%matplotlib inline\n",
    "\n",
    "#import toy accelerator package\n",
    "from uspas_ml.accelerator_toy_models import simple_lattices\n",
    "from uspas_ml.utils import transformer\n",
    "\n",
    "# Gaussian process\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "# Neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data\n",
    "\n",
    "We start by generating noisy data from a model of beam propagating through a single quad magnet (characterized by a magnetic strength `K1`). In order to simulate fluctuations in beam parameters and noise in the beam size measurement, we add noise in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xrms_1D(K1, noise=1.5e-3):\n",
    "    '''\n",
    "    calculate sigma_x^2\n",
    "    \n",
    "    K1 : magnetic strength of single quad magnet, torch.tensor, shape (1,)\n",
    "    noise : rms noise level of measurement, float\n",
    "    '''\n",
    "    #generate initial beam matrix\n",
    "    # - x/y/z geometric emittance of 1.0e-8 m-rad\n",
    "    init_beam_matrix = torch.eye(6) * 1.0e-8\n",
    "\n",
    "    #set x_rms beam size to 1 mm and rms divergence to 0.1 mrad\n",
    "    init_beam_matrix[0,0] = 1.0e-3 ** 2 \n",
    "    init_beam_matrix[1,1] = 1.0e-4 ** 2 \n",
    "    init_beam_matrix[2,2] = 1.0e-3 ** 2 \n",
    "    init_beam_matrix[3,3] = 1.0e-4 ** 2  \n",
    "    \n",
    "    #create accelerator lattice object with one quad and a drift\n",
    "    line = simple_lattices.create_singlet(K1)\n",
    "    \n",
    "    #propagate beam matrix\n",
    "    final_beam_matrix = line.propagate_beam_matrix(init_beam_matrix, noise)\n",
    "    return final_beam_matrix[0,0].reshape(1,1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 400 samples for K1 varying between -300 and 300\n",
    "n_samples = 400\n",
    "bounds = torch.tensor([-300.0, 300.0])\n",
    "train_x = torch.rand(n_samples) * (bounds[1] - bounds[0]) + bounds[0]\n",
    "train_x = train_x.reshape(-1,1)\n",
    "train_y = torch.cat([xrms_1D(K) for K in train_x]).detach()\n",
    "\n",
    "# Normalize the generated data (scale input between 0 and 1 ; set output to have std of 1)\n",
    "transformer_x = transformer.Transformer(bounds.reshape(2,1), transform_type = 'normalize')\n",
    "normed_train_x = transformer_x.forward(train_x)\n",
    "\n",
    "transformer_y = transformer.Transformer(train_y, transform_type = 'standardize')\n",
    "normed_train_y = transformer_y.forward(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( normed_train_x, normed_train_y, '.' )\n",
    "plt.xlabel('Normalized magnetic strength K1')\n",
    "plt.ylabel('Standardized beam size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian process\n",
    "\n",
    "Here we fit a Gaussian process to the data, we draw a few samples from the GP and plot them, along with the uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Gaussian Process model with Matern(5/2) kernel and Gaussian likelihood\n",
    "gp = SingleTaskGP(normed_train_x, normed_train_y)\n",
    "\n",
    "# Train model hyperparameters by minimizing negative-Log-Marginal-Likelihood\n",
    "mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "fit_gpytorch_model(mll);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original training data\n",
    "plt.plot( normed_train_x, normed_train_y, '.', alpha=0.3 )\n",
    "\n",
    "# Draw samples of the GP, and plot them over the interval -0.5, 1.5\n",
    "x = torch.linspace(-0.5, 1.5, 200).reshape(-1,1)\n",
    "p = gp.posterior(x, observation_noise=False )\n",
    "\n",
    "for _ in range(10):\n",
    "    sample, = p.rsample()\n",
    "    plt.plot( x.flatten(), sample.detach().numpy().flatten(), alpha=0.5 )\n",
    "\n",
    "# Plot the analytical uncertainty\n",
    "l,u = p.mvn.confidence_region()\n",
    "plt.fill_between( x.flatten(), l.detach().numpy(), u.detach().numpy(), color='b', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Normalized magnetic strength K1')\n",
    "plt.ylabel('Standardized beam size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** \n",
    "    Does the uncertainty behave as expected? In which way? Does the predicted uncertainty \n",
    "    capture the aleatoric part or only the epistemic part? If it does not capture the aleatoric part, \n",
    "    modify (slightly) the above code that generates samples from the Gaussian Process, in such a way that it \n",
    "    does capture it. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here** (Does the uncertainty behave as expected? In which way? Does the predicted uncertainty \n",
    "    capture the aleatoric part or only the epistemic part?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: copy the above cell and modify it so as to capture the aleatoric part of the uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble of neural networks\n",
    "\n",
    "We will now train an **ensemble** of neural networks on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Define fully-connected neural network with 2 hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(1, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(10, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_train):\n",
    "        x = self.layer(x_train)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_train_model( epochs=100, plot_loss=True ):\n",
    "    \"\"\"\n",
    "    Train a neural network, optionally plot the loss during training \n",
    "    (to check convergence), and return the corresponding neural network object\n",
    "    \"\"\"\n",
    "    model = NNModel()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "    \n",
    "    loss_list = []\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        output = model(normed_train_x)\n",
    "        loss = nn.MSELoss()(output, normed_train_y)\n",
    "        loss_list.append( float(loss) )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if plot_loss:\n",
    "        plt.plot( loss_list )\n",
    "        plt.xlabel( 'Epoch' )\n",
    "        plt.ylabel( 'Training Loss' )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 10 different neural network on the same data\n",
    "ensemble_models = [ generate_and_train_model() for _ in range(10) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples_with_mean_plusminus_std(x, y_samples, show_samples=True):\n",
    "    \"\"\"\n",
    "    Helper function that plots samples, along with the mean +/- std\n",
    "    \n",
    "    x: numpy array of size (n,)\n",
    "        points on the horizontal axis\n",
    "        \n",
    "    y_samples: list of numpy arrays of size (n,)\n",
    "        the corresponding samples \n",
    "\n",
    "    show_samples: bool\n",
    "        whether to show the samples or only the mean and std\n",
    "    \"\"\"\n",
    "    # Plot samples\n",
    "    if show_samples:\n",
    "        for i in range(len(y_samples)):\n",
    "            plt.plot(x, y_samples[i], alpha=0.5)\n",
    "    \n",
    "    # Calculate mean and std\n",
    "    y = np.array(y_samples)\n",
    "    mean = np.mean(y,axis=0)\n",
    "    std = np.std(y,axis=0)\n",
    "    plt.plot( x, mean, 'b' )\n",
    "    plt.fill_between( x, mean-std, mean+std, color='b', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** \n",
    "    Modify the code below to plot sample predictions from the ensemble of neural network, and to plot the mean and uncertainty. \n",
    "    Since each neural network has been trained on the same dataset, why do they \n",
    "    give different predictions? Does the resulting uncertainty capture the aleatoric part?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original training data\n",
    "plt.plot( normed_train_x, normed_train_y, '.', alpha=0.3 )\n",
    "\n",
    "# Plot predictions of the neural networks over the interval -0.5, 1.5\n",
    "x = torch.linspace(-0.5, 1.5, 200).reshape(-1,1)\n",
    "\n",
    "y_samples = []\n",
    "# Your code here: capture the prediction of each neural network in the list `y_samples`\n",
    "\n",
    "    \n",
    "plot_samples_with_mean_plusminus_std(x.flatten(), y_samples, show_samples=True)\n",
    "\n",
    "plt.xlabel('Normalized magnetic strength K1')\n",
    "plt.ylabel('Standardized beam size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:** (Since each neural network has been trained on the same dataset, why do they \n",
    "    give different predictions? Does the resulting uncertainty capture the aleatoric part?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Dropout\n",
    "\n",
    "We now use a Monte-Carlo drop-out neural network to try to assess the uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropoutModel(nn.Module):\n",
    "\n",
    "    def __init__(self, p):\n",
    "        \"\"\"\n",
    "        Initialize a Monte-Carlo drop-out neural network\n",
    "        \n",
    "        p: float\n",
    "            Drop-out probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(1, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p), # Sets activations to 0 with probability p\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p), # Sets activations to 0 with probability p\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p), # Sets activations to 0 with probability p        \n",
    "            nn.Linear(10, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_train):\n",
    "        x = self.layer(x_train)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_train_dropout_model( p=0.1, epochs=100, plot_loss=True ):\n",
    "    \"\"\"\n",
    "    Train an MC drop-out neural network, optionally plot the loss during training \n",
    "    (to check convergence), and return the corresponding neural network object\n",
    "    \"\"\"\n",
    "    model = MCDropoutModel(p)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "    \n",
    "    loss_list = []\n",
    "    for epoch in tqdm.tqdm(range(epochs)):    \n",
    "        output = model(normed_train_x)\n",
    "        loss = nn.MSELoss()(output, normed_train_y)\n",
    "        loss_list.append( float(loss) )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if plot_loss:\n",
    "        plt.plot( loss_list )\n",
    "        plt.xlabel( 'Epoch' )\n",
    "        plt.ylabel( 'Training loss' )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network in the presence of Drop-out\n",
    "mcdropout_model = generate_and_train_dropout_model(epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell several times. \n",
    "# The result changes everytime, because different activation neurons are set to 0 (with probability p).\n",
    "# This gives the uncertainty on the prediction.\n",
    "mcdropout_model( torch.tensor([[0.3]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** \n",
    "    By modifying the code below, generate 10 different sample predictions with the MC dropout model, and plot the corresponding uncertainty.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original training data\n",
    "plt.plot( normed_train_x, normed_train_y, '.', alpha=0.3 )\n",
    "\n",
    "# Plot predictions of the neural networks over the interval -0.5, 1.5\n",
    "x = torch.linspace(-0.5, 1.5, 200).reshape(-1,1)\n",
    "\n",
    "y_samples = []\n",
    "# Your code here: Collect 10 different sample predictions\n",
    "\n",
    "plot_samples_with_mean_plusminus_std(x.flatten(), y_samples, show_samples=False)\n",
    "\n",
    "plt.xlabel('Normalized magnetic strength K1')\n",
    "plt.ylabel('Standardized beam size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** \n",
    "    Execute the cell below, and try to plot again the uncertainty. \n",
    "    (e.g. by copying the above cell and executing it) What happens and why?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcdropout_model.eval(); # Set in evaluation mode (this can be reverted by doing `mcdropout_model.train()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:** (What happens and why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile regression\n",
    "\n",
    "We will now evaluate the uncertainty using quantile regression.\n",
    "\n",
    "In quantile regression, we instantiate a regular neural network, but train it with specific loss function (see the slides) instead of the usual Mean Squared Error loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** \n",
    "    Implement the quantile regression loss function in the cell below (see course slides), and then implement the function `generate_and_train_quantil_model` to train a quantile regression neural network with this loss function\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss_function(y_pred, y_true, quantile):\n",
    "    \"\"\"\n",
    "    Return the loss function for quantile regression\n",
    "    \n",
    "    y_pred: array of shape (n,1)\n",
    "        predictions of the machine learning model\n",
    "         \n",
    "    y_true: array of shape (n,1)\n",
    "        data labels\n",
    "        \n",
    "    quantile: float\n",
    "        (number between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Your code below: modify the lines below to defined the quantile loss function\n",
    "    # (The current code implements the MSE loss function instead.)\n",
    "    \n",
    "    error = y_true - y_pred\n",
    "    loss = torch.mean( error**2, axis=0 )\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_train_quantile_model( quantile, epochs=100, plot_loss=True ):\n",
    "    \"\"\"\n",
    "    Train an quantile neural network, optionally plot the loss during training \n",
    "    (to check convergence), and return the corresponding neural network object\n",
    "    \n",
    "    quantile: float\n",
    "        (number between 0 and 1)\n",
    "    \"\"\"\n",
    "    model = NNModel() # Use a regular neural network\n",
    "    \n",
    "    # Your code here: train the neural network with `quantile_loss_function`\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a few quantile values, and train a neural networks for each of them\n",
    "quantiles = [0.2, 0.5, 0.9]\n",
    "quantile_models = [ generate_and_train_quantile_model(q, epochs=200) for q in quantiles ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original training data\n",
    "plt.plot( normed_train_x, normed_train_y, '.', alpha=0.3 )\n",
    "\n",
    "# Plot predictions of the neural networks over the interval -0.5, 1.5\n",
    "x = torch.linspace(-0.5, 1.5, 200).reshape(-1,1)\n",
    "\n",
    "for model, q in zip(quantile_models, quantiles):\n",
    "    plt.plot( x, model(x).detach().numpy(), label=q )\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.xlabel('Normalized magnetic strength K1')\n",
    "plt.ylabel('Standardized beam size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** \n",
    "    Are the relative positions of the 0.2, 0.5 and 0.9 curves as expected? \n",
    "    Does this capture the aleatoric part of the uncertainty?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian neural networks\n",
    "\n",
    "We will now use the `blitz` package to train bayesian neural networks. `blitz` combines with standard `pytorch` neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian neural network\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a bayesian neural network with two hidden layers (with 10 neurons each) \n",
    "\n",
    "# - We use `BayesianLinear` instead of `Linear`. \n",
    "#     `BayesianLinear` holds `mu` and `rho` (for each neuron) as trainable parameter, instead of the weights directly.\n",
    "#     Its output is random, as it draws random weights for each evaluation\n",
    "# - `variational_estimator` enables additional methods for the class, including the ELBO loss function\n",
    "\n",
    "@variational_estimator\n",
    "class BayesianModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            BayesianLinear(1, 10, prior_sigma_1=1., prior_sigma_2=1.e-6, prior_pi=0.5),\n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(10, 10, prior_sigma_1=1., prior_sigma_2=1.e-6, prior_pi=0.5),\n",
    "            nn.ReLU(),        \n",
    "            BayesianLinear(10, 1, prior_sigma_1=1., prior_sigma_2=1.e-6, prior_pi=0.5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_train):\n",
    "        x = self.layer(x_train)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_train_bayesian_model( epochs=1000, plot_loss=True ):\n",
    "    \"\"\"\n",
    "    Train a Bayesian neural network, optionally plot the loss during training \n",
    "    (to check convergence), and return the corresponding neural network object\n",
    "    \"\"\"\n",
    "    model = BayesianModel()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "    \n",
    "    loss_list = []\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        \n",
    "        loss = model.sample_elbo(inputs=normed_train_x,\n",
    "                                 labels=normed_train_y,\n",
    "                                 criterion=nn.MSELoss(),\n",
    "                                 sample_nbr=2,\n",
    "                                 complexity_cost_weight=1./normed_train_x.shape[0])\n",
    "        loss_list.append( float(loss) )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if plot_loss:\n",
    "        plt.plot( loss_list )\n",
    "        plt.xlabel( 'Epoch' )\n",
    "        plt.ylabel( 'Training loss' )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "bayesian_model = generate_and_train_bayesian_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** \n",
    "    By modifying the code below, plot the uncertainty for the Bayesian neural network.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original training data\n",
    "plt.plot( normed_train_x, normed_train_y, '.', alpha=0.3 )\n",
    "\n",
    "# Plot predictions of the neural networks over the interval -0.5, 1.5\n",
    "x = torch.linspace(-0.5, 1.5, 200).reshape(-1,1)\n",
    "\n",
    "y_samples = []\n",
    "# Your code here: Collect 10 different sample predictions\n",
    "\n",
    "\n",
    "plot_samples_with_mean_plusminus_std(x.flatten(), y_samples, show_samples=True)\n",
    "\n",
    "plt.xlabel('Normalized magnetic strength K1')\n",
    "plt.ylabel('Standardized beam size')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
